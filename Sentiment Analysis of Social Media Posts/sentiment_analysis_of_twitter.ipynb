{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["flld9wUNWtE-","Zy-l2q8gZM64","zhZsWb-WXSZ0","8385DEM3Vu__","IZPCNl0KapUA"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2231927,"sourceType":"datasetVersion","datasetId":1340873}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohsinmshabbir/twitter-sentiment-analysis-traditional-dl?scriptVersionId=187135419\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Setting Up","metadata":{"id":"UmfKeVKhZ-LR"}},{"cell_type":"markdown","source":"## Setting Up Conda Environment","metadata":{}},{"cell_type":"code","source":"# conda create -n nlp2 python=3.10.*\n# conda activate nlp2 \n# conda install tensorflow\n# conda install pandas\n# conda install spacy\n# conda install scikit-learn\n# conda install imbalanced-learn\n# conda install worldcloud\n# conda install gensim","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{"id":"QygXDrytWUOU"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport spacy\nimport string\nimport random\n\nfrom wordcloud import WordCloud # type: ignore\nimport matplotlib.pyplot as plt # type: ignore\n#! python -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.dummy import DummyClassifier","metadata":{"id":"TjWdtmfcUzYR","is_executing":true,"outputId":"f486035f-b937-437c-c7db-8ae4468d5607"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! pip install keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\n\nimport gensim # type: ignore\nfrom gensim.models import Word2Vec # type: ignore","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Data","metadata":{"id":"NJBg8oOfWfij"}},{"cell_type":"code","source":"df = pd.read_csv('Twitter_Data.csv')\ndf.head()","metadata":{"id":"52ZT3ZKsWYjJ","outputId":"28cc5484-1901-46c2-bfa8-c914ce81ab44"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{"id":"d5F_exxsWjqG"}},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.category.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['clean_text'][19])\nprint(df['clean_text'][91])","metadata":{"id":"Kw5PX1OwHEWQ","outputId":"acaedd24-9aa3-415a-9a07-4bd7bf329080"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"any_nan_in_A = df['clean_text'].isna().any()\nprint(any_nan_in_A)","metadata":{"id":"n2aS5NlUXCCB","outputId":"cb3d12dd-5c80-4656-9495-242c71828d7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text'] = df['clean_text'].fillna('')","metadata":{"id":"qaOSdm4FXNZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()","metadata":{"id":"atozIrJflBnd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"any_nan_in_A = df['category'].isna().any()\nprint(any_nan_in_A)","metadata":{"id":"LRjG2kycJJnm","outputId":"2e717bf9-5b06-4a2d-9fd1-f6405592f320"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'^[^ ]<.*?>|&([a-z0-9]+|#[0-9]\\\"\\'\\â€œ{1,6}|#x[0-9a-f]{1,6});[^A-Za-z0-9]+')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\ndef remove_quotes(text):\n    quotes = re.compile(r'[^A-Za-z0-9\\s]+')\n    return re.sub(quotes, '', text)\n\n\n# Applying helper functions\ndf1 = df.copy()\ndf1['clean_text'] = df1['clean_text'].apply(lambda x: remove_URL(x))\ndf1['clean_text'] = df1['clean_text'].apply(lambda x: remove_emoji(x))\ndf1['clean_text'] = df1['clean_text'].apply(lambda x: remove_html(x))\ndf1['clean_text'] = df1['clean_text'].apply(lambda x: remove_punct(x))\ndf1['clean_text'] = df1['clean_text'].apply(lambda x: remove_quotes(x))\ndf1['clean_text'] = df1['clean_text'].str.lower()\n\n\nprint(df1['clean_text'][19]),\nprint(df1['clean_text'][91])","metadata":{"id":"umT9OUwsjNxR","outputId":"4712e890-3ee4-431b-a1c6-7d371c09c289"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reducing the Dataset Size\n\nAs the Dataset size is too large, we'll reduce the dataset size by dropping the neutral (0.0) sentiment score in our Dataset","metadata":{}},{"cell_type":"code","source":"df2 =df1.copy()\ndf2 = df1[df1.category != 0.0]\ndf2.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = df2.drop_duplicates(subset=['clean_text'])\ndf2.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reduced the dataset Size by 34% approx.","metadata":{}},{"cell_type":"code","source":"df3 = df2.copy()\ndef clean_tweet_text(text):\n  doc = nlp(text)\n  tokens = [token.lemma_ for token in doc if not token.is_digit\n            and not token.is_space]\n  return \" \".join(tokens)\n\ndf3['clean_text'] = df3[\"clean_text\"].apply(clean_tweet_text)\n\nprint(df3['clean_text'][19]),\nprint(df3['clean_text'][91])","metadata":{"id":"pYNxVcLZgvLl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratary Data Analysis","metadata":{"id":"flld9wUNWtE-"}},{"cell_type":"code","source":"df_test = df3.copy()\ncategory = df_test['category']\nmapped_labels = [0 if label == -1 else 1 for label in category]\ndf_test['category'] = mapped_labels\ndf_test.head()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_test.describe())\n\n# Print the info\nprint(df_test.info())","metadata":{"id":"Jp_YnhqnEINh","outputId":"5731791c-5e82-4d66-d340-2bab13db6ba8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate sentiment counts\nsentiment_counts = df_test['category'].value_counts()\n\n# Get values for x-axis and y-axis\nx = sentiment_counts.index.astype(str).to_list()  # Ensure string labels for x-axis\ny = sentiment_counts.values.tolist()\n\n# Create the bar chart\nplt.bar(x, y)\n\n# Customize the plot\nplt.title(\"Sentiment Distribution\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n\n# Adjust layout for better visibility\nplt.tight_layout()\n\n# Display the plot\nplt.show()","metadata":{"id":"0jcFrPM7EaxD","outputId":"492bb0a5-8ecd-4732-a709-1581417badc5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**\n\nPositive data is the larger than negative.\n\nWhich means our data is not balanced. We'll keep that in my mind when we'll train and test our data.","metadata":{"id":"9Oy9PckEe8RX"}},{"cell_type":"code","source":"df4 = df_test.copy()\ndf4['word_count'] = df_test['clean_text'].apply(lambda x : len(x.split()))\ndf4['char_count'] = df_test['clean_text'].apply(lambda x : len(x.replace(\" \",\"\")))\ndf4['word_density'] = df4['word_count'] / (df4['char_count'] + 1)\n\ndf4[['word_count', 'char_count', 'word_density']].head()","metadata":{"id":"qDBwJGB-YKHW","outputId":"b5012501-9321-4a04-eb32-2862e82eec2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_distribution_by_category(df, column, start, end, size, category_type):\n    # Filter data based on category\n    negative_df = df[df['category'] == 0]\n    positive_df = df[df['category'] == 1]\n\n    # Define bins for histogram\n    bins = np.arange(start, end + size, size)\n\n    # Create subplots with 1 row and 2 columns (for negative, positive)\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))  # Adjust figure size as needed\n\n    # Plot histograms for each category in separate subplots\n    ax[0].hist(negative_df[column], bins=bins, color='orange', alpha=0.75)\n    ax[0].set_title('Negative Tweets')\n    ax[0].set_xlabel(f'Tweet Length {category_type}')\n    ax[0].set_ylabel('Number of Tweets')\n\n    ax[1].hist(positive_df[column], bins=bins, color='blue', alpha=0.75)\n    ax[1].set_title('Positive Tweets')\n    ax[1].set_xlabel(f'Tweet Length {category_type}')\n    ax[1].set_ylabel('Number of Tweet')\n\n    # Adjust layout to prevent overlap of labels\n    plt.tight_layout()\n\n    # Show plot\n    plt.show()","metadata":{"id":"uwBuu0RFWv9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution_by_category(df4, 'word_count', 0, 60, 3,'Words')","metadata":{"id":"iRmPGpADKaGW","outputId":"180ce013-1b1e-4356-e30a-1903e2cfc55d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**\n\n> Most tweets prefer to use less than 40 words to write a tweet.\n>\n> Generally, tweeters write about 10 - 37 words\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"Tg_iWbrlbI2n"}},{"cell_type":"code","source":"plot_distribution_by_category(df4, 'char_count', 0, 300, 30,'Characters')","metadata":{"id":"DtumnI9kKy4l","outputId":"e85580e0-e284-481c-bd44-21e6405c38ab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference**\n\n\n> The twitter has a limitation of 280 words and yet tweeters use anywhere between 50-200 characters\n>\n\n\n\n","metadata":{"id":"RJ1sNms-dF2Q"}},{"cell_type":"code","source":"plot_distribution_by_category(df4,'word_density', 0.09, 0.3, .01,'Word density')","metadata":{"id":"4cD_yFMGLN_Q","outputId":"9db5de8f-ce18-4e8b-e335-a8fadad99f69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_tweets = df4[df4['category'] == 1.0]\nnegative_tweets = df4[df4['category'] == 0.0]","metadata":{"id":"l5V6f3YbQoFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df4.head()","metadata":{"id":"dyxJEe0tGb2S","outputId":"fe9009fe-b7ef-4557-917b-34c4a1ee4a45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_text_cln = \" \".join(positive_tweets.clean_text)\nneg_text_cln = \" \".join(negative_tweets.clean_text)\n\n# replacing some most common words present in these texts\nnoise_words = ['will', 'make', 'people','say', 'vote', 'now', 'give',\n               's', 'one', 'govt', 'thi', 'hi', 'ju', 'hi'\n               ]\nfor noise in noise_words:\n    pos_text_cln = pos_text_cln.lower().replace(noise,\" \")\n    neg_text_cln = neg_text_cln.lower().replace(noise, \" \")\n\ndef green_color(word, font_size, position, orientation, random_state=None, **kwargs):\n    return 'hsl({:d}, 80%, {:d}%)'.format(random.randint(85, 140), random.randint(60, 80))\n\ndef red_color(word, font_size, position, orientation, random_state=None, **kwargs):\n    return 'hsl({:d}, 80%, {:d}%)'.format(random.randint(0, 35), random.randint(60, 80))\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 6])\n\nwordcloud1 = WordCloud(background_color='white', height=400).generate(pos_text_cln)\nax1.imshow(wordcloud1.recolor(color_func=green_color, random_state=3),interpolation=\"bilinear\")\nax1.axis('off');\nax1.set_title('Positive Tweets');\n\nwordcloud2 = WordCloud(background_color='white', height=400).generate(neg_text_cln)\nax2.imshow(wordcloud2.recolor(color_func=red_color, random_state=3),interpolation=\"bilinear\")\nax2.axis('off');\nax2.set_title('Negative Tweets');","metadata":{"id":"TojEvwbyQ-9V","outputId":"3c9d895e-adb6-4fab-93a1-be4848ce1271"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference\n\n> One thing is clear that our data comes largely from India as evident from WordCloud\n>\n> The topic also seems to be around politics primarily talking about 'bjp', 'congress' and 'modi'\n>\n> The word 'modi', prime minister of India, is used in every sentiment indicating both support and opposition for \"Modi Government\".","metadata":{"id":"dOGAIin4o9FE"}},{"cell_type":"markdown","source":"# Saving the dataset","metadata":{"id":"Zy-l2q8gZM64"}},{"cell_type":"code","source":"df4.to_csv('refined_tweet_data.csv',encoding = 'utf-8-sig')","metadata":{"id":"F2JX5rJhZL4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"code","source":"X = df4['clean_text']\nY = df4['category']","metadata":{"id":"1YPFPLNsXNEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,\n                                                    stratify = Y,\n                                                    random_state = 34)","metadata":{"id":"nahAm6hsMeUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialize the tf-id vectorizer\nvectorizer = TfidfVectorizer(strip_accents='ascii')\n\n#Using the vectorizer to fit on out training data and testing data\ntfidf_train = vectorizer.fit_transform(X_train)\ntfidf_test = vectorizer.transform(X_test)","metadata":{"id":"0WwwZP9lMyAK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a Majority Class Classifier\nmajority_classifier = DummyClassifier(strategy=\"most_frequent\")\nmajority_classifier.fit(tfidf_train, Y_train)\n\n# Predict the majority class for all instances in the test set\ny_pred_majority = majority_classifier.predict(tfidf_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(Y_test, y_pred_majority))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Traditional Machine Learning Model","metadata":{"id":"ZYC2IXGRWy42"}},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{"id":"s9VinFq-XOHV"}},{"cell_type":"code","source":"# df4 = pd.read_csv('refined_tweet_data.csv')","metadata":{"id":"raWhtlZVjxVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialize the Multinomial Naive Bayes classifier\nbest_accuracy = 0\na = 0.1\nwhile a <= 1:\n  nb = MultinomialNB(alpha=a)\n\n  #Fitting the model\n  nb.fit(tfidf_train, Y_train)\n  curr_accuracy = nb.score(tfidf_test, Y_test)\n  print(f\"Accuracy with alpha {a}: {curr_accuracy}\")\n  # Predict the labels\n  if curr_accuracy > best_accuracy:\n    best_accuracy = curr_accuracy\n    y_pred = nb.predict(tfidf_test)\n  a = a + 0.05","metadata":{"id":"yopTwo-qO25S","outputId":"168c91d0-71e0-4a46-a157-da35dd340e6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate Performance","metadata":{"id":"cTMe_ilLXpu0"}},{"cell_type":"code","source":"# Print classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(Y_test, y_pred))","metadata":{"id":"ymNzlYRNPQIS","outputId":"99de2de1-6f5e-45f4-b8fb-e4f4b08b8f89"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote = SMOTE(random_state=42)\ntfidf_train_oversample, Y_train = smote.fit_resample(tfidf_train, Y_train)","metadata":{"id":"VKH817qBd5LU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialize the Multinomial Naive Bayes classifier\nbest_accuracy = 0\na = 0.1\nwhile a <= 1:\n  nb = MultinomialNB(alpha=a)\n\n  #Fitting the model\n  nb.fit(tfidf_train_oversample, Y_train)\n  curr_accuracy = nb.score(tfidf_test, Y_test)\n  print(f\"Accuracy with alpha {a}: {curr_accuracy}\")\n  # Predict the labels\n  if curr_accuracy > best_accuracy:\n    best_accuracy = curr_accuracy\n    y_pred = nb.predict(tfidf_test)\n  a = a + 0.05","metadata":{"id":"VmH9Rk_OeP-N","outputId":"51c8a3af-ba2b-4a5e-96bb-4030d1537153"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print classification report\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(Y_test, y_pred))","metadata":{"id":"XWPJu6ZxfVNB","outputId":"5a6115c9-7a78-4646-d679-ef842618b0f7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference:**\n\nBased on the evaluation metrics, the text classification model demonstrates a GOOD overall performance. It achieves an accuracy of almost 81%, with somewhat imbalanced precision and precise recall for both classes. The F1-score further support this conclusion.","metadata":{"id":"88R4Gle9X42T"}},{"cell_type":"markdown","source":"### Prediction using Naive Bayes","metadata":{}},{"cell_type":"code","source":"def predict_tweet(tweet, model, vectorizer):\n    # Preprocess the input tweet\n    tweet = remove_URL(tweet)\n    tweet = remove_emoji(tweet)\n    tweet = remove_html(tweet)\n    tweet = remove_punct(tweet)\n    tweet = remove_quotes(tweet)\n    tweet = tweet.lower()\n    tweet = clean_tweet_text(tweet)\n    \n    # Transform the tweet using the TF-IDF vectorizer\n    tweet_tfidf = vectorizer.transform([tweet])\n    \n    # Predict the label\n    prediction = model.predict(tweet_tfidf)\n    return prediction[0]\n\nnew_text = \"Great match tonight! Our team played exceptionally well and secured a decisive victory. #football #winning\"\npredicted_sentiment= predict_tweet(new_text, nb, vectorizer)\n\n# Print the result with the sentiment label\nsentiment_label = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\nprint(f\"Predicted Sentiment: {sentiment_label}, Score: {predicted_sentiment}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{"id":"zhZsWb-WXSZ0"}},{"cell_type":"code","source":"logistic_regression = LogisticRegression(max_iter=1000)\nlogistic_regression.fit(tfidf_train_oversample, Y_train)\n\nlogistic_prediction = logistic_regression.predict(tfidf_test)\nlogistic_accuracy = accuracy_score(Y_test, logistic_prediction)\nprint(f\"Accuracy: {logistic_accuracy}\")","metadata":{"id":"8K_qSLpNXWBE","outputId":"39acb7aa-449d-4068-99d1-92f0e173329a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate Performance","metadata":{"id":"I0ru0q07X1cU"}},{"cell_type":"code","source":"# Print the Classification Report\ncr = classification_report(Y_test, logistic_prediction)\nprint(\"\\n\\nClassification Report\\n\")\nprint(cr)","metadata":{"id":"xjoY0_VtX4Ap","outputId":"2b6d9506-1ac7-4d88-99b9-c5f72e08aa33"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference:**\n\nThese metrics suggest that the text classification model demonstrates great overall performance with an accuracy of 91% in classifying text data. It can effectively identify positive and negative tweets with a balanced accuracy and has a good ability to differentiate between the classes.","metadata":{"id":"mu31DTHUovOy"}},{"cell_type":"markdown","source":"### Prediction using Logistic Regression","metadata":{}},{"cell_type":"code","source":"new_text = \"Great match tonight! Our team played exceptionally well and secured a decisive victory. #football #winning\"\npredicted_sentiment= predict_tweet(new_text, logistic_regression, vectorizer)\n\n# Print the result with the sentiment label\nsentiment_label = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\nprint(f\"Predicted Sentiment: {sentiment_label}, Score: {predicted_sentiment}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec Analysis","metadata":{"id":"Vx2Vz1zuX9ys"}},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\n\n# Parameters for WORD2VEC\nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\n\n# Parameters related to KERAS\nSEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024","metadata":{"id":"pJNBPY0ANlup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = train_test_split(df4, test_size=1-TRAIN_SIZE, random_state=42)","metadata":{"id":"AbrETXeVM5PM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Corpus Creation","metadata":{"id":"o9dNN0kBYL9x"}},{"cell_type":"code","source":"documents = [text.split() for text in df_train.clean_text]","metadata":{"id":"lXR6-jtMKzUt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word2Vec Model Creation","metadata":{"id":"v8TPKoBkYSR8"}},{"cell_type":"code","source":"w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE,\n                                            window=W2V_WINDOW,\n                                            min_count=W2V_MIN_COUNT,\n                                            workers=8)","metadata":{"id":"WJrSA_g3LH4G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vocabulary Creation","metadata":{"id":"xj5tisIsYZgt"}},{"cell_type":"code","source":"w2v_model.build_vocab(documents)","metadata":{"id":"IrZdlAN5Nv93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = w2v_model.wv.key_to_index.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","metadata":{"id":"yUMA-W6GNw0C","outputId":"122fe38f-4028-44db-859c-fd6d9732dbcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Word2Vec Model","metadata":{"id":"dq6e2EfPYgBY"}},{"cell_type":"code","source":"w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","metadata":{"id":"Ra8j87zbN7ZZ","outputId":"e390db52-ce8c-4f8a-c308-7af605af9418"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word2Vec Model Testing","metadata":{"id":"yoVqK5PeYlSo"}},{"cell_type":"code","source":"w2v_model.wv.most_similar(\"india\")","metadata":{"id":"G2N6knU2O0u_","outputId":"eb9822fc-a961-48a8-b68d-fe682c9e97d0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar(\"bjp\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar(\"narendra\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning Models","metadata":{"id":"gM2hZpALY-ci"}},{"cell_type":"markdown","source":"## LSTM","metadata":{"id":"lpkAqLILY1Qy"}},{"cell_type":"code","source":"def create_tokenizer_and_vocab(df_train, text_column):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(df_train[text_column])\n\n    vocab_size = len(tokenizer.word_index) + 1\n    print(\"Total words\", vocab_size)\n\n    return tokenizer, vocab_size\n\ndef preprocess_texts(texts, tokenizer, sequence_length):\n    text_sequences = tokenizer.texts_to_sequences(texts)\n    text_padded = pad_sequences(text_sequences, maxlen=sequence_length)\n    return text_padded\n\ndef decode_sentiment(score):       \n    return 0 if score <= 0.5 else 1\n\ndef predict_sentiment(text, model, tokenizer, sequence_length):\n    text_padded = preprocess_texts(text, tokenizer, sequence_length)\n    score = model.predict(text_padded, verbose=0)[0][0]\n    sentiment = decode_sentiment(score)\n    return sentiment, score\n\n\ntokenizer, vocab_size = create_tokenizer_and_vocab(df_train, 'clean_text')","metadata":{"id":"__Jx30sjPrVV","outputId":"971b92d5-306e-4ca9-f261-2ce7a493b169"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Embedding Matrix","metadata":{}},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n  if word in w2v_model.wv:\n    embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","metadata":{"id":"MG8N_a75U1xr","outputId":"3c96cb7d-e209-4627-c151-21d36d697c44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)","metadata":{"id":"lMcS1KAKU22K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM Model Creation","metadata":{"id":"PrKQ6TY0ZBbc"}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"id":"P22GqRuCU5pV","outputId":"14f357cb-9715-497b-d5fb-a05db1419fbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","metadata":{"id":"9-VNn_VtVFbQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing training and test data","metadata":{}},{"cell_type":"code","source":"# Prepare training and test data\nx_train = preprocess_texts(df_train.clean_text, tokenizer, SEQUENCE_LENGTH)\nx_test = preprocess_texts(df_test.clean_text, tokenizer, SEQUENCE_LENGTH)\ny_train = df_train['category'].values.astype('int32').reshape(-1, 1)\ny_test = df_test['category'].values.astype('int32').reshape(-1, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM Model Training","metadata":{"id":"n0Bp_gIUZHb1"}},{"cell_type":"code","source":"callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5)]","metadata":{"id":"SlfmsGIHVIrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_history = model.fit(x_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_split=0.1,\n                    verbose=1,\n                    callbacks=callbacks)","metadata":{"id":"QJSt86t1VMcO","outputId":"628dcf42-662f-4ebd-a4c0-c9ac1c128341"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating the Model (LSTM)","metadata":{"id":"V07xCY2vYZ2k"}},{"cell_type":"code","source":"score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Predictions","metadata":{}},{"cell_type":"code","source":"# Making predictions on test set\ny_pred_1d = [decode_sentiment(score) for score in model.predict(x_test, verbose=1, batch_size=8000)]\ny_test_1d = list(df_test['category'])\nprint(classification_report(y_test_1d, y_pred_1d))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting sentiment of a new text\nnew_text = \"The movie had stunning visuals and a great soundtrack, but the plot was incredibly boring and the acting was subpar.\"\npredicted_sentiment, score = predict_sentiment(new_text, model, tokenizer, SEQUENCE_LENGTH)\n\n\n# Print the result with the sentiment label\nsentiment_label = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\nprint(f\"Predicted Sentiment: {sentiment_label}, Score: {score}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNNs","metadata":{"id":"Vl_Ip_OHYUFz"}},{"cell_type":"markdown","source":"### Hyperparameters","metadata":{"id":"dRXV3-FObaUi"}},{"cell_type":"code","source":"# Hyperparameters (you can adjust these)\nFILTER_SIZES = [3, 4, 5]  # Experiment with different filter sizes\nNUM_FILTERS = 128  # Number of filters per convolutional layer\nSEQUENCE_LENGTH = 300  # Assuming tweets are preprocessed to this length\nEPOCHS = 8\nBATCH_SIZE = 1024","metadata":{"id":"vtxfxp9ebNqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size, embedding_dim = embedding_matrix.shape","metadata":{"id":"bWwZ6bgobeWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define CNN Model","metadata":{"id":"5GbmKfClc_gM"}},{"cell_type":"code","source":"cnn_model = Sequential()\n\n# Embedding layer with pre-trained weights (non-trainable)\ncnn_model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False))\n\n# Convolutional layers with different filter sizes\nfor filter_size in FILTER_SIZES:\n  cnn_model.add(Conv1D(NUM_FILTERS, kernel_size=filter_size, activation='relu'))\n  cnn_model.add(MaxPooling1D(pool_size=2))\n\n# Flatten layer and add fully-connected layers\ncnn_model.add(Flatten())\ncnn_model.add(Dense(128, activation='relu'))\ncnn_model.add(Dropout(0.5))\ncnn_model.add(Dense(1, activation='sigmoid'))  # Binary-class classification (positive, negative)\n\ncnn_model.summary()","metadata":{"id":"mxF2C1X1bnVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lr_schedule(epoch):\n  ##\"\"\"Reduce learning rate by 10% every 2 epochs.\"\"\"\n  lr = 0.001  # Initial learning rate\n  if epoch > 0 and epoch % 2 == 0:\n    lr *= 0.9\n  return lr\n\nlearning_rate_scheduler = LearningRateScheduler(lr_schedule)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compile and train CNN Model","metadata":{"id":"a94eOKksdJSP"}},{"cell_type":"code","source":"cnn_model.compile(loss='binary_crossentropy',\n                 optimizer=\"adam\",\n                 metrics=['accuracy'])\n\ncnn_history = cnn_model.fit(x_train, y_train,\n                             batch_size=BATCH_SIZE,\n                             epochs=EPOCHS,\n                             validation_split=0.1,\n                             verbose=1,\n                             callbacks=[learning_rate_scheduler]\n                             )","metadata":{"id":"b2Ylp7EWdMCS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate Performance(CNN)","metadata":{"id":"Rz1KGFQgYbgC"}},{"cell_type":"code","source":"test_loss, test_accuracy = cnn_model.evaluate(x_test, y_test, verbose=1)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Predictions","metadata":{}},{"cell_type":"code","source":"cnn_scores = cnn_model.predict(x_test, verbose=1, batch_size=8000)\ncnn_y_pred_1d = [decode_sentiment(score[0]) for score in cnn_scores]\nprint(classification_report(y_test_1d, cnn_y_pred_1d))","metadata":{"id":"diW8vWZoVKB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_text = \"The movie had stunning visuals and a great soundtrack, but the plot was incredibly boring and the acting was subpar.\"\npredicted_sentiment, score = predict_sentiment(new_text, cnn_model, tokenizer, SEQUENCE_LENGTH)\n\n# Print the result with the sentiment label\nsentiment_label = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\nprint(f\"Predicted Sentiment: {sentiment_label}, Score: {score}\")","metadata":{},"execution_count":null,"outputs":[]}]}